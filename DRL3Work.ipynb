{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qy8Kim8aTUhk"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIdIEKMHWbiK",
    "outputId": "16e90b9e-9565-4f84-d814-e9c20aa31480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "try:\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        LOAD = True\n",
    "        SAVE_MODEL_PATH = '/content/gdrive/MyDrive/models/' + 'q_network'\n",
    "    else:\n",
    "        LOAD = False\n",
    "        SAVE_MODEL_PATH = \"./models/q_network\"\n",
    "except NameError:\n",
    "        LOAD = False\n",
    "        SAVE_MODEL_PATH = \"./models/q_network\"\n",
    "\n",
    "\n",
    "use_cuda = True\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "if use_cuda:\n",
    "    criterion = nn.MSELoss().cuda()   \n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "           # transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))  #  numbers here need to be adjusted in future\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q0sD2HLnWbsB"
   },
   "outputs": [],
   "source": [
    "#import torchvision.datasets.SBDataset as sbd\n",
    "import os\n",
    "import imageio\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import cv2 as cv\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, classe, alpha=0.2, nu=3.0, threshold=0.5, num_episodes=15, load=False ):\n",
    "        \"\"\"\n",
    "            Classe initialisant l'ensemble des paramètres de l'apprentissage, un agent est associé à une classe donnée du jeu de données.\n",
    "        \"\"\"\n",
    "        self.BATCH_SIZE = 100\n",
    "        self.GAMMA = 0.900\n",
    "        self.EPS = 1\n",
    "        self.TARGET_UPDATE = 1\n",
    "        self.save_path = SAVE_MODEL_PATH\n",
    "        screen_height, screen_width = 224, 224\n",
    "        self.n_actions = 9\n",
    "        self.classe = classe\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        if not load:\n",
    "            self.policy_net = DQN(screen_height, screen_width, self.n_actions)\n",
    "        else:\n",
    "            self.policy_net = self.load_network()\n",
    "            \n",
    "        self.target_net = DQN(screen_height, screen_width, self.n_actions)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        if use_cuda:\n",
    "          self.feature_extractor = self.feature_extractor.cuda()\n",
    "          self.target_net = self.target_net.cuda()\n",
    "          self.policy_net = self.policy_net.cuda()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(),lr=1e-6)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        \n",
    "        self.alpha = alpha # €[0, 1]  Scaling factor\n",
    "        self.nu = nu # Reward of Trigger\n",
    "        self.threshold = threshold\n",
    "        self.actions_history = []\n",
    "        self.num_episodes = num_episodes\n",
    "        self.actions_history += [[100]*9]*20\n",
    "\n",
    "    def save_network(self):\n",
    "        \"\"\"\n",
    "            Fonction de sauvegarde du Q-Network\n",
    "        \"\"\"\n",
    "        torch.save(self.policy_net, self.save_path+\"_\"+self.classe)\n",
    "        print('Saved')\n",
    "\n",
    "    def load_network(self):\n",
    "        \"\"\"\n",
    "            Récupération d'un Q-Network existant\n",
    "        \"\"\"\n",
    "        if not use_cuda:\n",
    "            return torch.load(self.save_path+\"_\"+self.classe, map_location=torch.device('cpu'))\n",
    "        return torch.load(self.save_path+\"_\"+self.classe)\n",
    "\n",
    "\n",
    "\n",
    "    def intersection_over_union(self, box1, box2):\n",
    "        \"\"\"\n",
    "            Calcul de la mesure d'intersection/union\n",
    "            Entrée :\n",
    "                Coordonnées [x_min, x_max, y_min, y_max] de la boite englobante de la vérité terrain et de la prédiction\n",
    "            Sortie :\n",
    "                Score d'intersection/union.\n",
    "\n",
    "        \"\"\"\n",
    "        x11, x21, y11, y21 = box1\n",
    "        x12, x22, y12, y22 = box2\n",
    "        \n",
    "        yi1 = max(y11, y12)\n",
    "        xi1 = max(x11, x12)\n",
    "        yi2 = min(y21, y22)\n",
    "        xi2 = min(x21, x22)\n",
    "        inter_area = max(((xi2 - xi1) * (yi2 - yi1)), 0)\n",
    "        box1_area = (x21 - x11) * (y21 - y11)\n",
    "        box2_area = (x22 - x12) * (y22 - y12)\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        iou = float(inter_area) / float(union_area)\n",
    "        return iou\n",
    "\n",
    "    def compute_reward(self, actual_state, previous_state, ground_truth):\n",
    "        \"\"\"\n",
    "            Calcul la récompense à attribuer pour les états non-finaux selon les cas.\n",
    "            Entrée :\n",
    "                Etats actuels et précédents ( coordonnées de boite englobante )\n",
    "                Coordonnées de la vérité terrain\n",
    "            Sortie :\n",
    "                Récompense attribuée\n",
    "        \"\"\"\n",
    "        res = self.intersection_over_union(actual_state, ground_truth) - self.intersection_over_union(previous_state, ground_truth)\n",
    "        if res <= 0:\n",
    "            return -1\n",
    "        return 1\n",
    "      \n",
    "    def rewrap(self, coord):\n",
    "        return min(max(coord,0), 224)\n",
    "      \n",
    "    def compute_trigger_reward(self, actual_state, ground_truth):\n",
    "        \"\"\"\n",
    "            Calcul de la récompensée associée à un état final selon les cas.\n",
    "            Entrée :\n",
    "                Etat actuel et boite englobante de la vérité terrain\n",
    "            Sortie : \n",
    "                Récompense attribuée\n",
    "        \"\"\"\n",
    "        res = self.intersection_over_union(actual_state, ground_truth)\n",
    "        if res>=self.threshold:\n",
    "            return self.nu\n",
    "        return -1*self.nu\n",
    "\n",
    "    def get_best_next_action(self, actions, ground_truth):\n",
    "        \"\"\"\n",
    "            Implémentation de l'Agent expert qui selon l'état actuel et la vérité terrain va donner la meilleur action possible.\n",
    "            Entrée :\n",
    "                - Liste d'actions executées jusqu'à présent.\n",
    "                - Vérité terrain.\n",
    "            Sortie :\n",
    "                - Indice de la meilleure action possible.\n",
    "\n",
    "        \"\"\"\n",
    "        max_reward = -99\n",
    "        best_action = -99\n",
    "        positive_actions = []\n",
    "        negative_actions = []\n",
    "        actual_equivalent_coord = self.calculate_position_box(actions)\n",
    "        for i in range(0, 9):\n",
    "            copy_actions = actions.copy()\n",
    "            copy_actions.append(i)\n",
    "            new_equivalent_coord = self.calculate_position_box(copy_actions)\n",
    "            if i!=0:\n",
    "                reward = self.compute_reward(new_equivalent_coord, actual_equivalent_coord, ground_truth)\n",
    "            else:\n",
    "                reward = self.compute_trigger_reward(new_equivalent_coord,  ground_truth)\n",
    "            \n",
    "            if reward>=0:\n",
    "                positive_actions.append(i)\n",
    "            else:\n",
    "                negative_actions.append(i)\n",
    "        if len(positive_actions)==0:\n",
    "            return random.choice(negative_actions)\n",
    "        return random.choice(positive_actions)\n",
    "\n",
    "\n",
    "    def select_action(self, state, actions, ground_truth):\n",
    "        \"\"\"\n",
    "            Selection de l'action dépendemment de l'état\n",
    "            Entrée :\n",
    "                - Etat actuel. \n",
    "                - Vérité terrain.\n",
    "            Sortie :\n",
    "                - Soi l'action qu'aura choisi le modèle soi la meilleure action possible ( Le choix entre les deux se fait selon un jet aléatoire ).\n",
    "        \"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    inpu = Variable(state).cuda()\n",
    "                else:\n",
    "                    inpu = Variable(state)\n",
    "                qval = self.policy_net(inpu)\n",
    "                _, predicted = torch.max(qval.data,1)\n",
    "                action = predicted[0] # + 1\n",
    "                try:\n",
    "                  return action.cpu().numpy()[0]\n",
    "                except:\n",
    "                  return action.cpu().numpy()\n",
    "        else:\n",
    "            #return np.random.randint(0,9)   # Avant implémentation d'agent expert\n",
    "            return self.get_best_next_action(actions, ground_truth) # Appel à l'agent expert.\n",
    "\n",
    "    def select_action_model(self, state):\n",
    "        \"\"\"\n",
    "            Selection d'une action par le modèle selon l'état\n",
    "            Entrée :\n",
    "                - Etat actuel ( feature vector / sortie du réseau convolutionnel + historique des actions )\n",
    "            Sortie :\n",
    "                - Action séléctionnée.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "                if use_cuda:\n",
    "                    inpu = Variable(state).cuda()\n",
    "                else:\n",
    "                    inpu = Variable(state)\n",
    "                qval = self.policy_net(inpu)\n",
    "                _, predicted = torch.max(qval.data,1)\n",
    "                #print(\"Predicted : \"+str(qval.data))\n",
    "                action = predicted[0] # + 1\n",
    "                #print(action)\n",
    "                return action\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"\n",
    "        Fonction effectuant les étapes de mise à jour du réseau ( sampling des épisodes, calcul de loss, rétro propagation )\n",
    "        \"\"\"\n",
    "        # Si la taille actuelle de notre mémoire est inférieure aux batchs de mémoires qu'on veut prendre en compte on n'effectue\n",
    "        # Pas encore d'optimization\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Extraction d'un echantillon aléatoire de la mémoire ( ou chaque éléments est constitué de (état, nouvel état, action, récompense) )\n",
    "        # Et ce pour éviter le biais occurant si on apprenait sur des états successifs\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Séparation des différents éléments contenus dans les différents echantillons\n",
    "        non_final_mask = torch.Tensor(tuple(map(lambda s: s is not None, batch.next_state))).bool()\n",
    "        next_states = [s for s in batch.next_state if s is not None]\n",
    "        non_final_next_states = Variable(torch.cat(next_states), \n",
    "                                         volatile=True).type(Tensor)\n",
    "        \n",
    "        state_batch = Variable(torch.cat(batch.state)).type(Tensor)\n",
    "        if use_cuda:\n",
    "            state_batch = state_batch.cuda()\n",
    "        action_batch = Variable(torch.LongTensor(batch.action).view(-1,1)).type(LongTensor)\n",
    "        reward_batch = Variable(torch.FloatTensor(batch.reward).view(-1,1)).type(Tensor)\n",
    "\n",
    "\n",
    "        # Passage des états par le Q-Network ( en calculate Q(s_t, a) ) et on récupére les actions sélectionnées\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Calcul de V(s_{t+1}) pour les prochain états.\n",
    "        next_state_values = Variable(torch.zeros(self.BATCH_SIZE, 1).type(Tensor)) \n",
    "\n",
    "        if use_cuda:\n",
    "            non_final_next_states = non_final_next_states.cuda()\n",
    "        \n",
    "        # Appel au second Q-Network ( celui de copie pour garantir la stabilité de l'apprentissage )\n",
    "        d = self.target_net(non_final_next_states) \n",
    "        next_state_values[non_final_mask] = d.max(1)[0].view(-1,1)\n",
    "        next_state_values.volatile = False\n",
    "\n",
    "        # On calcule les valeurs de fonctions Q attendues ( en faisant appel aux récompenses attribuées )\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "        # Calcul de la loss\n",
    "        loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        # Rétro-propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    \n",
    "    def compose_state(self, image, dtype=FloatTensor):\n",
    "        \"\"\"\n",
    "            Composition d'un état : Feature Vector + Historique des actions\n",
    "            Entrée :\n",
    "                - Image ( feature vector ). \n",
    "            Sortie :\n",
    "                - Représentation d'état.\n",
    "        \"\"\"\n",
    "        image_feature = self.get_features(image, dtype)\n",
    "        image_feature = image_feature.view(1,-1)\n",
    "        #print(\"image feature : \"+str(image_feature.shape))\n",
    "        history_flatten = self.actions_history.view(1,-1).type(dtype)\n",
    "        state = torch.cat((image_feature, history_flatten), 1)\n",
    "        return state\n",
    "    \n",
    "    def get_features(self, image, dtype=FloatTensor):\n",
    "        \"\"\"\n",
    "            Extraction du feature vector à partir de l'image.\n",
    "            Entrée :\n",
    "                - Image\n",
    "            Sortie :\n",
    "                - Feature vector\n",
    "        \"\"\"\n",
    "        global transform\n",
    "        #image = transform(image)\n",
    "        image = image.view(1,*image.shape)\n",
    "        image = Variable(image).type(dtype)\n",
    "        if use_cuda:\n",
    "            image = image.cuda()\n",
    "        feature = self.feature_extractor(image)\n",
    "        #print(\"Feature shape : \"+str(feature.shape))\n",
    "        return feature.data\n",
    "\n",
    "    \n",
    "    def update_history(self, action):\n",
    "        \"\"\"\n",
    "            Fonction qui met à jour l'historique des actions en y ajoutant la dernière effectuée\n",
    "            Entrée :\n",
    "                - Dernière action effectuée\n",
    "        \"\"\"\n",
    "        action_vector = torch.zeros(9)\n",
    "        action_vector[action] = 1\n",
    "        size_history_vector = len(torch.nonzero(self.actions_history))\n",
    "        if size_history_vector < 9:\n",
    "            self.actions_history[size_history_vector][action] = 1\n",
    "        else:\n",
    "            for i in range(8,0,-1):\n",
    "                self.actions_history[i][:] = self.actions_history[i-1][:]\n",
    "            self.actions_history[0][:] = action_vector[:] \n",
    "        return self.actions_history\n",
    "\n",
    "    def calculate_position_box(self, actions, xmin=0, xmax=224, ymin=0, ymax=224):\n",
    "        \"\"\"\n",
    "            Prends l'ensemble des actions depuis le début et en génére les coordonnées finales de la boite englobante.\n",
    "            Entrée :\n",
    "                - Ensemble des actions sélectionnées depuis le début.\n",
    "            Sortie :\n",
    "                - Coordonnées finales de la boite englobante.\n",
    "        \"\"\"\n",
    "        # Calcul des alpha_h et alpha_w mentionnées dans le papier\n",
    "        alpha_h = self.alpha * (  ymax - ymin )\n",
    "        alpha_w = self.alpha * (  xmax - xmin )\n",
    "        real_x_min, real_x_max, real_y_min, real_y_max = 0, 224, 0, 224\n",
    "\n",
    "        # Boucle sur l'ensemble des actions\n",
    "        for r in actions:\n",
    "            if r == 1: # Right\n",
    "                real_x_min += alpha_w\n",
    "                real_x_max += alpha_w\n",
    "            if r == 2: # Left\n",
    "                real_x_min -= alpha_w\n",
    "                real_x_max -= alpha_w\n",
    "            if r == 3: # Up \n",
    "                real_y_min -= alpha_h\n",
    "                real_y_max -= alpha_h\n",
    "            if r == 4: # Down\n",
    "                real_y_min += alpha_h\n",
    "                real_y_max += alpha_h\n",
    "            if r == 5: # Bigger\n",
    "                real_y_min -= alpha_h\n",
    "                real_y_max += alpha_h\n",
    "                real_x_min -= alpha_w\n",
    "                real_x_max += alpha_w\n",
    "            if r == 6: # Smaller\n",
    "                real_y_min += alpha_h\n",
    "                real_y_max -= alpha_h\n",
    "                real_x_min += alpha_w\n",
    "                real_x_max -= alpha_w\n",
    "            if r == 7: # Fatter\n",
    "                real_y_min += alpha_h\n",
    "                real_y_max -= alpha_h\n",
    "            if r == 8: # Taller\n",
    "                real_x_min += alpha_w\n",
    "                real_x_max -= alpha_w\n",
    "        real_x_min, real_x_max, real_y_min, real_y_max = self.rewrap(real_x_min), self.rewrap(real_x_max), self.rewrap(real_y_min), self.rewrap(real_y_max)\n",
    "        return [real_x_min, real_x_max, real_y_min, real_y_max]\n",
    "\n",
    "    def get_max_bdbox(self, ground_truth_boxes, actual_coordinates ):\n",
    "        \"\"\"\n",
    "            Récupére parmis les boites englobantes vérité terrain d'une image celle qui est la plus proche de notre état actuel.\n",
    "            Entrée :\n",
    "                - Boites englobantes des vérités terrain.\n",
    "                - Coordonnées actuelles de la boite englobante.\n",
    "            Sortie :\n",
    "                - Vérité terrain la plus proche.\n",
    "        \"\"\"\n",
    "        max_iou = False\n",
    "        max_gt = []\n",
    "        for gt in ground_truth_boxes:\n",
    "            iou = self.intersection_over_union(actual_coordinates, gt)\n",
    "            if max_iou == False or max_iou < iou:\n",
    "                max_iou = iou\n",
    "                max_gt = gt\n",
    "        return max_gt\n",
    "\n",
    "    def predict_image(self, image, plot=False):\n",
    "        \"\"\"\n",
    "            Prédit la boite englobante d'une image\n",
    "            Entrée :\n",
    "                - Image redimensionnée.\n",
    "            Sortie :\n",
    "                - Coordonnées boite englobante.\n",
    "        \"\"\"\n",
    "\n",
    "        # Passage du Q-Network en mode évaluation\n",
    "        self.policy_net.eval()\n",
    "        xmin = 0\n",
    "        xmax = 224\n",
    "        ymin = 0\n",
    "        ymax = 224\n",
    "\n",
    "        done = False\n",
    "        all_actions = []\n",
    "        self.actions_history = torch.ones((9,9))\n",
    "        state = self.compose_state(image)\n",
    "        original_image = image.clone()\n",
    "        new_image = image\n",
    "\n",
    "        steps = 0\n",
    "        \n",
    "        # Tant que le trigger n'est pas déclenché ou qu'on a pas atteint les 40 steps\n",
    "        while not done:\n",
    "            steps += 1\n",
    "            action = self.select_action_model(state)\n",
    "            all_actions.append(action)\n",
    "            if action == 0:\n",
    "                next_state = None\n",
    "                new_equivalent_coord = self.calculate_position_box(all_actions)\n",
    "                done = True\n",
    "            else:\n",
    "                # Mise à jour de l'historique\n",
    "                self.actions_history = self.update_history(action)\n",
    "                new_equivalent_coord = self.calculate_position_box(all_actions)            \n",
    "                \n",
    "                # Récupération du contenu de la boite englobante\n",
    "                new_image = original_image[:, int(new_equivalent_coord[2]):int(new_equivalent_coord[3]), int(new_equivalent_coord[0]):int(new_equivalent_coord[1])]\n",
    "                try:\n",
    "                    new_image = transform(new_image)\n",
    "                except ValueError:\n",
    "                    break            \n",
    "                \n",
    "                # Composition : état + historique des 9 dernières actions\n",
    "                next_state = self.compose_state(new_image)\n",
    "            \n",
    "            if steps == 40:\n",
    "                done = True\n",
    "            \n",
    "            # Déplacement au nouvel état\n",
    "            state = next_state\n",
    "            image = new_image\n",
    "        \n",
    "            if plot:\n",
    "                show_new_bdbox(original_image, new_equivalent_coord, color='b', count=steps)\n",
    "        \n",
    "\n",
    "        # Génération d'un GIF représentant l'évolution de la prédiction\n",
    "        if plot:\n",
    "            #images = []\n",
    "            tested = 0\n",
    "            while os.path.isfile('media/movie_'+str(tested)+'.gif'):\n",
    "                tested += 1\n",
    "            # filepaths\n",
    "            fp_out = \"media/movie_\"+str(tested)+\".gif\"\n",
    "            images = []\n",
    "            for count in range(1, steps+1):\n",
    "                images.append(imageio.imread(str(count)+\".png\"))\n",
    "            \n",
    "            imageio.mimsave(fp_out, images)\n",
    "            \n",
    "            for count in range(1, steps):\n",
    "                os.remove(str(count)+\".png\")\n",
    "        return new_equivalent_coord\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate(self, dataset):\n",
    "        \"\"\"\n",
    "            Evaluation des performances du model sur un jeu de données.\n",
    "            Entrée :\n",
    "                - Jeu de données de test.\n",
    "            Sortie :\n",
    "                - Statistiques d'AP et RECALL.\n",
    "\n",
    "        \"\"\"\n",
    "        ground_truth_boxes = []\n",
    "        predicted_boxes = []\n",
    "        print(\"Predicting boxes...\")\n",
    "        for key, value in dataset.items():\n",
    "            image, gt_boxes = extract(key, dataset)\n",
    "            bbox = self.predict_image(image)\n",
    "            ground_truth_boxes.append(gt_boxes)\n",
    "            predicted_boxes.append(bbox)\n",
    "        print(\"Computing recall and ap...\")\n",
    "        stats = eval_stats_at_threshold(predicted_boxes, ground_truth_boxes)\n",
    "        print(\"Final result : \\n\"+str(stats))\n",
    "        return stats\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        \"\"\"\n",
    "            Fonction d'entraînement du modèle.\n",
    "            Entrée :\n",
    "                - Jeu de données d'entraînement.\n",
    "        \"\"\"\n",
    "        xmin = 0.0\n",
    "        xmax = 224.0\n",
    "        ymin = 0.0\n",
    "        ymax = 224.0\n",
    "\n",
    "        for i_episode in range(self.num_episodes):\n",
    "            print(\"Episode \"+str(i_episode))\n",
    "            for key, value in  train_loader.items():\n",
    "                image, ground_truth_boxes = extract(key, train_loader)\n",
    "                original_image = image.clone()\n",
    "                ground_truth = ground_truth_boxes[0]\n",
    "                all_actions = []\n",
    "        \n",
    "                # Initialize the environment and state\n",
    "                self.actions_history = torch.ones((9,9))\n",
    "                state = self.compose_state(image)\n",
    "                original_coordinates = [xmin, xmax, ymin, ymax]\n",
    "                new_image = image\n",
    "                done = False\n",
    "                t = 0\n",
    "                actual_equivalent_coord = original_coordinates\n",
    "                new_equivalent_coord = original_coordinates\n",
    "                while not done:\n",
    "                    t += 1\n",
    "                    action = self.select_action(state, all_actions, ground_truth)\n",
    "                    all_actions.append(action)\n",
    "                    if action == 0:\n",
    "                        next_state = None\n",
    "                        new_equivalent_coord = self.calculate_position_box(all_actions)\n",
    "                        closest_gt = self.get_max_bdbox( ground_truth_boxes, new_equivalent_coord )\n",
    "                        reward = self.compute_trigger_reward(new_equivalent_coord,  closest_gt)\n",
    "                        done = True\n",
    "\n",
    "                    else:\n",
    "                        self.actions_history = self.update_history(action)\n",
    "                        new_equivalent_coord = self.calculate_position_box(all_actions)\n",
    "                        \n",
    "                        new_image = original_image[:, int(new_equivalent_coord[2]):int(new_equivalent_coord[3]), int(new_equivalent_coord[0]):int(new_equivalent_coord[1])]\n",
    "                        try:\n",
    "                            new_image = transform(new_image)\n",
    "                        except ValueError:\n",
    "                            break                        \n",
    "\n",
    "                        next_state = self.compose_state(new_image)\n",
    "                        closest_gt = self.get_max_bdbox( ground_truth_boxes, new_equivalent_coord )\n",
    "                        reward = self.compute_reward(new_equivalent_coord, actual_equivalent_coord, closest_gt)\n",
    "                        \n",
    "                        actual_equivalent_coord = new_equivalent_coord\n",
    "                    if t == 20:\n",
    "                        done = True\n",
    "                    self.memory.push(state, int(action), next_state, reward)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    state = next_state\n",
    "                    image = new_image\n",
    "                    # Perform one step of the optimization (on the target network)\n",
    "                    self.optimize_model()\n",
    "                    \n",
    "            \n",
    "            if i_episode % self.TARGET_UPDATE == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            if i_episode<5:\n",
    "                self.EPS -= 0.18\n",
    "            self.save_network()\n",
    "\n",
    "            print('Complete')\n",
    "\n",
    "    def train_validate(self, train_loader, valid_loader):\n",
    "        \"\"\"\n",
    "            Entraînement du modèle et à chaque épisode test de l'efficacité sur le jeu de test et sauvegarde des résultats dans un fichier de logs.\n",
    "        \"\"\"\n",
    "        op = open(\"logs_over_epochs\", \"w\")\n",
    "        op.write(\"NU = \"+str(self.nu))\n",
    "        op.write(\"ALPHA = \"+str(self.alpha))\n",
    "        op.write(\"THRESHOLD = \"+str(self.threshold))\n",
    "        xmin = 0.0\n",
    "        xmax = 224.0\n",
    "        ymin = 0.0\n",
    "        ymax = 224.0\n",
    "        for i_episode in range(self.num_episodes):  \n",
    "            print(\"Episode \"+str(i_episode))\n",
    "            for key, value in  train_loader.items():\n",
    "                image, ground_truth_boxes = extract(key, train_loader)\n",
    "                original_image = image.clone()\n",
    "                ground_truth = ground_truth_boxes[0]\n",
    "                all_actions = []\n",
    "        \n",
    "                # Initialize the environment and state\n",
    "                self.actions_history = torch.ones((9,9))\n",
    "                state = self.compose_state(image)\n",
    "                original_coordinates = [xmin, xmax, ymin, ymax]\n",
    "                new_image = image\n",
    "                done = False\n",
    "                t = 0\n",
    "                actual_equivalent_coord = original_coordinates\n",
    "                new_equivalent_coord = original_coordinates\n",
    "                while not done:\n",
    "                    t += 1\n",
    "                    action = self.select_action(state, all_actions, ground_truth)\n",
    "                    all_actions.append(action)\n",
    "                    if action == 0:\n",
    "                        next_state = None\n",
    "                        new_equivalent_coord = self.calculate_position_box(all_actions)\n",
    "                        closest_gt = self.get_max_bdbox( ground_truth_boxes, new_equivalent_coord )\n",
    "                        reward = self.compute_trigger_reward(new_equivalent_coord,  closest_gt)\n",
    "                        done = True\n",
    "\n",
    "                    else:\n",
    "                        self.actions_history = self.update_history(action)\n",
    "                        new_equivalent_coord = self.calculate_position_box(all_actions)\n",
    "                        \n",
    "                        new_image = original_image[:, int(new_equivalent_coord[2]):int(new_equivalent_coord[3]), int(new_equivalent_coord[0]):int(new_equivalent_coord[1])]\n",
    "                        try:\n",
    "                            new_image = transform(new_image)\n",
    "                        except ValueError:\n",
    "                            break                        \n",
    "                        if False:\n",
    "                            show_new_bdbox(original_image, ground_truth, color='r')\n",
    "                            show_new_bdbox(original_image, new_equivalent_coord, color='b')\n",
    "                            \n",
    "\n",
    "                        next_state = self.compose_state(new_image)\n",
    "                        closest_gt = self.get_max_bdbox( ground_truth_boxes, new_equivalent_coord )\n",
    "                        reward = self.compute_reward(new_equivalent_coord, actual_equivalent_coord, closest_gt)\n",
    "                        \n",
    "                        actual_equivalent_coord = new_equivalent_coord\n",
    "                    if t == 20:\n",
    "                        done = True\n",
    "                    self.memory.push(state, int(action), next_state, reward)\n",
    "\n",
    "                    # Vers le nouvel état\n",
    "                    state = next_state\n",
    "                    image = new_image\n",
    "                    # Optimisation\n",
    "                    self.optimize_model()\n",
    "                    \n",
    "            stats = self.evaluate(valid_loader)\n",
    "            op.write(\"\\n\")\n",
    "            op.write(\"Episode \"+str(i_episode))\n",
    "            op.write(str(stats))\n",
    "            if i_episode % self.TARGET_UPDATE == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            if i_episode<5:\n",
    "                self.EPS -= 0.18\n",
    "            self.save_network()\n",
    "            \n",
    "            print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afQ6udeRYRT_"
   },
   "source": [
    "Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y0rN54_qWbt6"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "PATH=\"./datasets/\"\n",
    "\n",
    "\n",
    "class CustomRotation(object):\n",
    "    \"\"\"\n",
    "        Fournit une classe trnaform qui remet les images dans la bonne orientation ( car mal orientées orignellement dans le jeu de données )\n",
    "    \"\"\"\n",
    "    def __call__(self, image):\n",
    "        return image.transpose(0, 2).transpose(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    \"\"\"\n",
    "        Permettant la préparation d'une fonction normalisation + le redimensionnement des images du jeu de données.\n",
    "    \"\"\"\n",
    "    base_size = 520\n",
    "    crop_size = 480\n",
    "\n",
    "    min_size = int((0.5 if train else 1.0) * base_size)\n",
    "    max_size = int((2.0 if train else 1.0) * base_size)\n",
    "    transf = []\n",
    "    transf.append( transforms.ToTensor())\n",
    "    transf.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225]))\n",
    "    return transforms.Compose(transf)\n",
    "    \n",
    "def make_image_transform(image_transform_params: dict,\n",
    "                         transform: object):\n",
    "    resize_image = image_transform_params['image_mode']\n",
    "    if resize_image == 'none':\n",
    "        preprocess_image = None\n",
    "    elif resize_image == 'shrink':\n",
    "        preprocess_image = transforms.Resize((image_transform_params['output_image_size']['width'],\n",
    "                                              image_transform_params['output_image_size']['height']))\n",
    "    elif resize_image == 'crop':\n",
    "        preprocess_image = transforms.CenterCrop((image_transform_params['output_image_size']['width'],\n",
    "                                                  image_transform_params['output_image_size']['height']))\n",
    "\n",
    "    if preprocess_image is not None:\n",
    "        if transform is not None:\n",
    "            image_transform = transforms.Compose([preprocess_image, transform])\n",
    "        else:\n",
    "            image_transform = preprocess_image\n",
    "    else:\n",
    "        image_transform = transform\n",
    "    return image_transform\n",
    "\n",
    "\n",
    "def read_voc_dataset(download=True, year='2012'):\n",
    "    \"\"\"\n",
    "        Fonction qui récupére les dataloaders de validation et de train du jeu de données PASCAL VOC selon l'année d'entrée\n",
    "\n",
    "    \"\"\"\n",
    "    T = transforms.Compose([\n",
    "                            transforms.Resize((224, 224)),\n",
    "                            transforms.ToTensor(),\n",
    "                             #CustomRotation()\n",
    "                            ])\n",
    "    voc_data =  torchvision.datasets.VOCDetection(PATH, year=year, image_set='train', \n",
    "                        download=download, transform=T)\n",
    "    train_loader = DataLoader(voc_data,shuffle=True)\n",
    "\n",
    "    voc_val =  torchvision.datasets.VOCDetection(PATH, year=year, image_set='val', \n",
    "                        download=download, transform=T)\n",
    "    val_loader = DataLoader(voc_val,shuffle=False)\n",
    "\n",
    "    return voc_data, voc_val\n",
    "\n",
    "def get_images_labels(dataloader):\n",
    "    \"\"\"\n",
    "        Récupére séparemment les images et labels du dataloader\n",
    "    \"\"\"\n",
    "    data_iter = iter(dataloader)\n",
    "    images, labels = next(data_iter)\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Fonctions permettant la récupération et lecture du jeu de données SB de Pytorch\n",
    "\"\"\"\n",
    "class NoisySBDataset():\n",
    "    def __init__(self, path, image_set=\"train\", transforms = None, download=True):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms\n",
    "        self.dataset = torchvision.datasets.SBDataset(root=path,\n",
    "                                                      image_set=image_set,\n",
    "                                                      download=download)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):  # a[x] for calling a.__getitem__(x)\n",
    "        img, truth = self.dataset[idx]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return (img, truth)\n",
    "\n",
    "\n",
    "def read_sbd_dataset(batch_size, download=True):\n",
    "    \"\"\"\n",
    "        Lecture et normalisation du jeu de données SB.\n",
    "    \"\"\"\n",
    "    T = transforms.Compose([\n",
    "                            transforms.Resize((224, 224)),\n",
    "                            transforms.ToTensor()\n",
    "                            ])\n",
    "    voc_data =  NoisySBDataset(PATH, image_set='train', \n",
    "                        download=download, transforms=T)\n",
    "    train_loader = DataLoader(voc_data, batch_size=32,shuffle=False,  collate_fn=lambda x: x)\n",
    "\n",
    "    voc_val =  NoisySBDataset(PATH, image_set='val', \n",
    "                        download=download, transforms=T)\n",
    "    val_loader = DataLoader(voc_val, batch_size=32,shuffle=False,  collate_fn=lambda x: x)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhGbw7bbYfVI"
   },
   "source": [
    "models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oILkmxORWbxi"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Modèle d'extraction de caractéristiques, ici en faisant appel à VGG-16.\n",
    "\"\"\"\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__() # recopier toute la partie convolutionnelle\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        vgg16.eval() # to not do dropout\n",
    "        self.features = list(vgg16.children())[0] \n",
    "        self.classifier = nn.Sequential(*list(vgg16.classifier.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"\n",
    "    Architecture du Q-Network comme décrite dans l'article.\n",
    "\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear( in_features= 81 + 25088, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear( in_features= 1024, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear( in_features= 1024, out_features=9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJIbz8mNYmFF"
   },
   "source": [
    "tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3d0zY2uOYlMM"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "classes = [ 'cat', 'dog', 'bird', 'motorbike','diningtable', 'train', 'tvmonitor', 'bus', 'horse', 'car', 'pottedplant', 'person', 'chair', 'boat', 'bottle', 'bicycle', 'aeroplane', 'cow', 'sheep', 'sofa']\n",
    "\n",
    "def sort_class_extract(datasets):    \n",
    "    \"\"\"\n",
    "        Permet le tri par classes d'un jeu de données, parcours l'ensemble des objets d'une image et si il trouve un objet d'une classe\n",
    "        il l'ajoute au jeu de données de celle-ci. \n",
    "        Entrée :\n",
    "            - Jeu de données. \n",
    "        Sortie :\n",
    "            - Dictionnaire de jeu de données. ( clés : Classes, valeurs : Toutes les données de cette classe )\n",
    "    \"\"\"\n",
    "    datasets_per_class = {}\n",
    "    for j in classes:\n",
    "        datasets_per_class[j] = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for i in dataset:\n",
    "            img, target = i\n",
    "            classe = target['annotation']['object'][0][\"name\"]\n",
    "            filename = target['annotation']['filename']\n",
    "\n",
    "            org = {}\n",
    "            for j in classes:\n",
    "                org[j] = []\n",
    "                org[j].append(img)\n",
    "            for i in range(len(target['annotation']['object'])):\n",
    "                classe = target['annotation']['object'][i][\"name\"]\n",
    "                org[classe].append(  [   target['annotation']['object'][i][\"bndbox\"], target['annotation']['size']   ]  )\n",
    "            \n",
    "            for j in classes:\n",
    "                if len( org[j] ) > 1:\n",
    "                    try:\n",
    "                        datasets_per_class[j][filename].append(org[j])\n",
    "                    except KeyError:\n",
    "                        datasets_per_class[j][filename] = []\n",
    "                        datasets_per_class[j][filename].append(org[j])       \n",
    "    return datasets_per_class\n",
    "\n",
    "\n",
    "def show_new_bdbox(image, labels, color='r', count=0):\n",
    "    \"\"\"\n",
    "        Fonction pour la visualisation des boites englobantes directement sur l'image.\n",
    "    \"\"\"\n",
    "    xmin, xmax, ymin, ymax = labels[0],labels[1],labels[2],labels[3]\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image.transpose(0, 2).transpose(0, 1))\n",
    "\n",
    "    width = xmax-xmin\n",
    "    height = ymax-ymin\n",
    "    rect = patches.Rectangle((xmin,ymin),width,height,linewidth=3,edgecolor=color,facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.set_title(\"Iteration \"+str(count))\n",
    "    plt.savefig(str(count)+'.png', dpi=100)\n",
    "\n",
    "\n",
    "def extract(index, loader):\n",
    "    \"\"\"\n",
    "        A partir du dataloader extrait ( et sépare ) les images et les boites englobantes vérité terrain\n",
    "        et adaptent les coordonnées par rapport aux nouvelles tailles d'images.\n",
    "    \"\"\"\n",
    "    extracted = loader[index]\n",
    "    ground_truth_boxes =[]\n",
    "    for ex in extracted:\n",
    "        img = ex[0]\n",
    "        bndbox = ex[1][0]\n",
    "        size = ex[1][1]\n",
    "        xmin = ( float(bndbox['xmin']) /  float(size['width']) ) * 224\n",
    "        xmax = ( float(bndbox['xmax']) /  float(size['width']) ) * 224\n",
    "\n",
    "        ymin = ( float(bndbox['ymin']) /  float(size['height']) ) * 224\n",
    "        ymax = ( float(bndbox['ymax']) /  float(size['height']) ) * 224\n",
    "\n",
    "        ground_truth_boxes.append([xmin, xmax, ymin, ymax])\n",
    "    return img, ground_truth_boxes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def voc_ap(rec, prec, voc2007=False):\n",
    "    \"\"\"\n",
    "        Calcul de l'AP et du Recall. Si voc2007 est vraie on utilise alors la mesure préconisé par le papier de PASCAL VOC 2007 ( méthode des 11 points )\n",
    "    \"\"\"\n",
    "    if voc2007:\n",
    "        ap = 0.0\n",
    "        for t in np.arange(0.0, 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap = ap + p / 11.0\n",
    "    else:\n",
    "        mrec = np.concatenate(([0.0], rec, [1.0]))\n",
    "        mpre = np.concatenate(([0.0], prec, [0.0]))\n",
    "\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def prec_rec_compute(bounding_boxes, gt_boxes, ovthresh):\n",
    "    \"\"\"\n",
    "        Calcul de précision et recall grâce à l'Intersection/Union et selon le threshold entre les vérités terrains et les prédictions.\n",
    "    \"\"\"\n",
    "    nd = len(bounding_boxes)\n",
    "    npos = nd\n",
    "    tp = np.zeros(nd)\n",
    "    fp = np.zeros(nd)\n",
    "    d = 0\n",
    "\n",
    "    for index in range(len(bounding_boxes)):\n",
    "        box1 = bounding_boxes[index]\n",
    "        box2 = gt_boxes[index][0]\n",
    "        x11, x21, y11, y21 = box1[0], box1[1], box1[2], box1[3]\n",
    "        x12, x22, y12, y22 = box2[0], box2[1], box2[2], box2[3]\n",
    "        \n",
    "        yi1 = max(y11, y12)\n",
    "        xi1 = max(x11, x12)\n",
    "        yi2 = min(y21, y22)\n",
    "        xi2 = min(x21, x22)\n",
    "        inter_area = max(((xi2 - xi1) * (yi2 - yi1)), 0)\n",
    "        #  Union(A,B) = A + B - Inter(A,B)\n",
    "        box1_area = (x21 - x11) * (y21 - y11)\n",
    "        box2_area = (x22 - x12) * (y22 - y12)\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        # Calcul de IOU\n",
    "        iou = inter_area / union_area\n",
    "\n",
    "        if iou > ovthresh:\n",
    "            tp[d] = 1.0\n",
    "        else:            \n",
    "            fp[d] = 1.0\n",
    "        d += 1\n",
    "        \n",
    "    \n",
    "    # Calcul de la précision et du recall\n",
    "    fp = np.cumsum(fp)\n",
    "    tp = np.cumsum(tp)\n",
    "    rec = tp / float(npos)\n",
    "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "    \n",
    "    return prec, rec\n",
    "\n",
    "\n",
    "def compute_ap_and_recall(all_bdbox, all_gt, ovthresh):\n",
    "    \"\"\"\n",
    "        Calcul de la VOC detection metrique. \n",
    "    \"\"\"\n",
    "    prec, rec = prec_rec_compute(all_bdbox, all_gt, ovthresh)\n",
    "    ap = voc_ap(rec, prec, False)\n",
    "    return ap, rec[-1]\n",
    "\n",
    "\n",
    "def eval_stats_at_threshold( all_bdbox, all_gt, thresholds=[0.4, 0.5, 0.6, 0.7, 0.8]):\n",
    "    \"\"\"\n",
    "        Evaluation et collecte des statistiques et ce pour différents seuils.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for ovthresh in thresholds:\n",
    "        ap, recall = compute_ap_and_recall(all_bdbox, all_gt, ovthresh)\n",
    "        stats[ovthresh] = {'ap': ap, 'recall': recall}\n",
    "    stats_df = pd.DataFrame.from_records(stats)*100\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Structure de données pour stocker les éléments de mémoire pour l'algorithme de Replay Memory.\n",
    "\"\"\"\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94v35sf3YxLp"
   },
   "source": [
    "Training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "101945fb7b344c628b8e680e9f3f63ca",
      "b3c2c1823ccd406cb79c69888668b0cf",
      "36870e5d110347f8af38ba54545747d7",
      "5b9c78ece7a640a2ba754be44815e940",
      "2b0c6c12e938499b8e4c192080be5a9a",
      "0ed79390650a49c295cebb9d180a3fb1",
      "a777bcbca46c4e59964e55b4a30e698a",
      "1be85b898bc24808a4ce47f2ab4a0e49",
      "6575a82ea898441eac4659ba455ab73b",
      "905d068a90ee4d6488082911eae185ba",
      "f83afe1dcdb24e1a92cfbeeee3ce6974"
     ]
    },
    "id": "geIwVW6DYle8",
    "outputId": "89f1b533-f4ab-413d-c371-50d580eae088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./datasets/VOCtrainval_11-May-2012.tar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101945fb7b344c628b8e680e9f3f63ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999639040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/VOCtrainval_11-May-2012.tar to ./datasets/\n",
      "Using downloaded and verified file: ./datasets/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./datasets/VOCtrainval_11-May-2012.tar to ./datasets/\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "import tqdm.notebook as tq\n",
    "import seaborn as sns\n",
    "\n",
    "#!pip3 install torch==1.5.1 torchvision==0.6.1 -f https://download.pytorch.org/whl/cu92/torch_stable.html\n",
    "try:\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        LOAD = True\n",
    "        SAVE_MODEL_PATH = '/content/gdrive/MyDrive/models/' + 'q_network'\n",
    "    else:\n",
    "        LOAD = False\n",
    "        SAVE_MODEL_PATH = \"./models/q_network\"\n",
    "except NameError:\n",
    "        LOAD = False\n",
    "        SAVE_MODEL_PATH = \"./models/q_network\"\n",
    "batch_size = 32\n",
    "PATH=\"./datasets/\"\n",
    "\n",
    "\n",
    "\n",
    "train_loader2012, val_loader2012 = read_voc_dataset(download=LOAD, year='2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4BblxyoqaJoa"
   },
   "outputs": [],
   "source": [
    "classes = [ 'cat', 'dog', 'bird', 'motorbike','diningtable', 'train', 'tvmonitor', 'bus', 'horse', 'car', 'pottedplant', 'person', 'chair', 'boat', 'bottle', 'bicycle', 'aeroplane', 'cow', 'sheep', 'sofa']\n",
    "agents_per_class = {}\n",
    "datasets_per_class = sort_class_extract([train_loader2012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "87f27b6298c24971b15241e7f680b303",
      "6108f97cd6bf430c9baf48b3d073b230",
      "65ac2d3ca3674e1a8da58e41c534c57c",
      "3e22a5b915274c8aa93b041d7013c39c",
      "7bbdf022e0674abdb6cde134d673c956",
      "eec6f9b2752b44199063de8477cc15e3",
      "d5a13ae6c3584de9a824c57b9e339eb8",
      "3630eff3310e4753aadf205630adb301",
      "15ad3cb3a6eb4ab1a88ff498d6819513",
      "9e6c276ba81f45cba58b80a9be929594",
      "48fc63be454b42898b08826fd7a58dba"
     ]
    },
    "id": "Y-OmVfBsaYR5",
    "outputId": "f76bc933-ad8e-4428-e6f8-e38e53bc97a3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f27b6298c24971b15241e7f680b303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe cat...\n",
      "Episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-3f294565e578>:239: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  non_final_next_states = Variable(torch.cat(next_states),\n",
      "<ipython-input-9-3f294565e578>:261: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  next_state_values.volatile = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "Complete\n",
      "Episode 1\n",
      "Saved\n",
      "Complete\n",
      "Episode 2\n",
      "Saved\n",
      "Complete\n",
      "Episode 3\n",
      "Saved\n",
      "Complete\n",
      "Episode 4\n",
      "Saved\n",
      "Complete\n",
      "Episode 5\n",
      "Saved\n",
      "Complete\n",
      "Episode 6\n",
      "Saved\n",
      "Complete\n",
      "Episode 7\n",
      "Saved\n",
      "Complete\n",
      "Episode 8\n",
      "Saved\n",
      "Complete\n",
      "Episode 9\n",
      "Saved\n",
      "Complete\n",
      "Episode 10\n",
      "Saved\n",
      "Complete\n",
      "Episode 11\n",
      "Saved\n",
      "Complete\n",
      "Episode 12\n",
      "Saved\n",
      "Complete\n",
      "Episode 13\n",
      "Saved\n",
      "Complete\n",
      "Episode 14\n",
      "Saved\n",
      "Complete\n",
      "Classe dog...\n",
      "Episode 0\n",
      "Saved\n",
      "Complete\n",
      "Episode 1\n",
      "Saved\n",
      "Complete\n",
      "Episode 2\n",
      "Saved\n",
      "Complete\n",
      "Episode 3\n",
      "Saved\n",
      "Complete\n",
      "Episode 4\n",
      "Saved\n",
      "Complete\n",
      "Episode 5\n",
      "Saved\n",
      "Complete\n",
      "Episode 6\n",
      "Saved\n",
      "Complete\n",
      "Episode 7\n",
      "Saved\n",
      "Complete\n",
      "Episode 8\n",
      "Saved\n",
      "Complete\n",
      "Episode 9\n",
      "Saved\n",
      "Complete\n",
      "Episode 10\n",
      "Saved\n",
      "Complete\n",
      "Episode 11\n",
      "Saved\n",
      "Complete\n",
      "Episode 12\n",
      "Saved\n",
      "Complete\n",
      "Episode 13\n",
      "Saved\n",
      "Complete\n",
      "Episode 14\n",
      "Saved\n",
      "Complete\n",
      "Classe motorbike...\n",
      "Episode 0\n",
      "Saved\n",
      "Complete\n",
      "Episode 1\n",
      "Saved\n",
      "Complete\n",
      "Episode 2\n",
      "Saved\n",
      "Complete\n",
      "Episode 3\n",
      "Saved\n",
      "Complete\n",
      "Episode 4\n",
      "Saved\n",
      "Complete\n",
      "Episode 5\n",
      "Saved\n",
      "Complete\n",
      "Episode 6\n",
      "Saved\n",
      "Complete\n",
      "Episode 7\n",
      "Saved\n",
      "Complete\n",
      "Episode 8\n",
      "Saved\n",
      "Complete\n",
      "Episode 9\n",
      "Saved\n",
      "Complete\n",
      "Episode 10\n",
      "Saved\n",
      "Complete\n",
      "Episode 11\n",
      "Saved\n",
      "Complete\n",
      "Episode 12\n",
      "Saved\n",
      "Complete\n",
      "Episode 13\n",
      "Saved\n",
      "Complete\n",
      "Episode 14\n",
      "Saved\n",
      "Complete\n",
      "Classe bus...\n",
      "Episode 0\n",
      "Saved\n",
      "Complete\n",
      "Episode 1\n",
      "Saved\n",
      "Complete\n",
      "Episode 2\n",
      "Saved\n",
      "Complete\n",
      "Episode 3\n",
      "Saved\n",
      "Complete\n",
      "Episode 4\n",
      "Saved\n",
      "Complete\n",
      "Episode 5\n",
      "Saved\n",
      "Complete\n",
      "Episode 6\n",
      "Saved\n",
      "Complete\n",
      "Episode 7\n",
      "Saved\n",
      "Complete\n",
      "Episode 8\n",
      "Saved\n",
      "Complete\n",
      "Episode 9\n",
      "Saved\n",
      "Complete\n",
      "Episode 10\n",
      "Saved\n",
      "Complete\n",
      "Episode 11\n",
      "Saved\n",
      "Complete\n",
      "Episode 12\n",
      "Saved\n",
      "Complete\n",
      "Episode 13\n",
      "Saved\n",
      "Complete\n",
      "Episode 14\n",
      "Saved\n",
      "Complete\n",
      "Classe aeroplane...\n",
      "Episode 0\n",
      "Saved\n",
      "Complete\n",
      "Episode 1\n",
      "Saved\n",
      "Complete\n",
      "Episode 2\n",
      "Saved\n",
      "Complete\n",
      "Episode 3\n",
      "Saved\n",
      "Complete\n",
      "Episode 4\n",
      "Saved\n",
      "Complete\n",
      "Episode 5\n",
      "Saved\n",
      "Complete\n",
      "Episode 6\n",
      "Saved\n",
      "Complete\n",
      "Episode 7\n",
      "Saved\n",
      "Complete\n",
      "Episode 8\n",
      "Saved\n",
      "Complete\n",
      "Episode 9\n",
      "Saved\n",
      "Complete\n",
      "Episode 10\n",
      "Saved\n",
      "Complete\n",
      "Episode 11\n",
      "Saved\n",
      "Complete\n",
      "Episode 12\n",
      "Saved\n",
      "Complete\n",
      "Episode 13\n",
      "Saved\n",
      "Complete\n",
      "Episode 14\n",
      "Saved\n",
      "Complete\n",
      "Classe horse...\n",
      "Episode 0\n",
      "Saved\n",
      "Complete\n",
      "Episode 1\n",
      "Saved\n",
      "Complete\n",
      "Episode 2\n",
      "Saved\n",
      "Complete\n",
      "Episode 3\n",
      "Saved\n",
      "Complete\n",
      "Episode 4\n",
      "Saved\n",
      "Complete\n",
      "Episode 5\n",
      "Saved\n",
      "Complete\n",
      "Episode 6\n",
      "Saved\n",
      "Complete\n",
      "Episode 7\n",
      "Saved\n",
      "Complete\n",
      "Episode 8\n",
      "Saved\n",
      "Complete\n",
      "Episode 9\n",
      "Saved\n",
      "Complete\n",
      "Episode 10\n",
      "Saved\n",
      "Complete\n",
      "Episode 11\n",
      "Saved\n",
      "Complete\n",
      "Episode 12\n",
      "Saved\n",
      "Complete\n",
      "Episode 13\n",
      "Saved\n",
      "Complete\n",
      "Episode 14\n",
      "Saved\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "data = ['cat', 'dog', 'motorbike', 'bus', 'aeroplane', 'horse']\n",
    "for i in tq.tqdm(range(len(data))):\n",
    "    classe = data[i]\n",
    "    print(\"Classe \"+str(classe)+\"...\")\n",
    "    agents_per_class[classe] = Agent(classe, alpha=0.2, num_episodes=15, load=False)\n",
    "    agents_per_class[classe].train(datasets_per_class[classe])\n",
    "    del agents_per_class[classe]\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nJ81suwOhPd0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import cv2 as cv\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import tqdm.notebook as tq\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "#!pip3 install torch==1.5.1 torchvision==0.6.1 -f https://download.pytorch.org/whl/cu92/torch_stable.html\n",
    "\n",
    "batch_size = 32\n",
    "PATH=\"./datasets/\"\n",
    "\n",
    "\n",
    "\n",
    "train_loader2012, val_loader2012 = read_voc_dataset(download=False, year='2012')\n",
    "\n",
    "classes = ['dog','cat', 'bird', 'motorbike', 'diningtable', 'train', 'tvmonitor', 'bus', 'horse', 'car', 'pottedplant', 'person', 'chair', 'boat', 'bottle', 'bicycle', 'dog', 'aeroplane', 'cow', 'sheep', 'sofa']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MSlKc1YahPnB"
   },
   "outputs": [],
   "source": [
    "agents_per_class = {}\n",
    "datasets_per_class = sort_class_extract([val_loader2012])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "95ce6449740c4052a6950825c6e24931",
      "ca04b6d3833b442b8fb523c05dff8063",
      "ed1f7bd45f594d5ca1faa5e804616631",
      "8f05fb20b7604c7383d8be1059598812",
      "2891f0026d6e431c8fc2f6d1999df657",
      "d68ea7a6295a4c3399f737ce1965d5c4",
      "620f011b1ff94c68980860c1687c496d",
      "80c25e1ee2ed4a30953faf72f2fd0d41",
      "120804e7627f44aa8e84cafb494edce7",
      "69e9daff5f494c63a9f27d02c69cc7ad",
      "56d8ed6d49114427a6c858ffe7df003f"
     ]
    },
    "id": "_poOrveuhpos",
    "outputId": "765c390f-28b7-42a9-9e31-f0735c36669f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ce6449740c4052a6950825c6e24931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class cat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting boxes...\n",
      "Computing recall and ap...\n",
      "Final result : \n",
      "              0.4        0.5        0.6        0.7        0.8\n",
      "ap      49.775926  36.688980  22.172852  12.257313   2.787248\n",
      "recall  69.301471  59.007353  45.955882  33.639706  15.808824\n",
      "Class dog...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting boxes...\n",
      "Computing recall and ap...\n",
      "Final result : \n",
      "              0.4        0.5        0.6        0.7        0.8\n",
      "ap      35.421743  21.837114  12.104507   5.394071   2.032898\n",
      "recall  59.304085  46.444781  34.341906  22.692890  13.161876\n",
      "Class motorbike...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting boxes...\n",
      "Computing recall and ap...\n",
      "Final result : \n",
      "              0.4        0.5        0.6        0.7       0.8\n",
      "ap      25.390173  17.993058  10.106953   4.124508  0.865441\n",
      "recall  50.000000  42.366412  31.679389  20.229008  9.160305\n",
      "Class bus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting boxes...\n",
      "Computing recall and ap...\n",
      "Final result : \n",
      "              0.4        0.5        0.6        0.7       0.8\n",
      "ap      38.746901  23.893710  13.857681   6.649706  0.635619\n",
      "recall  60.189573  48.341232  36.492891  23.696682  7.109005\n",
      "Class aeroplane...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting boxes...\n",
      "Computing recall and ap...\n",
      "Final result : \n",
      "              0.4        0.5        0.6        0.7        0.8\n",
      "ap      35.938249  27.387151  12.291797   3.419320   1.330493\n",
      "recall  58.045977  49.425287  33.908046  18.103448  10.632184\n",
      "Class horse...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting boxes...\n",
      "Computing recall and ap...\n",
      "Final result : \n",
      "              0.4        0.5        0.6        0.7       0.8\n",
      "ap      31.810703  18.857668   9.907533   3.637889  1.383298\n",
      "recall  53.877551  42.040816  30.204082  16.734694  8.571429\n"
     ]
    }
   ],
   "source": [
    "data2 = ['cat', 'dog', 'motorbike', 'bus', 'aeroplane', 'horse']\n",
    "torch.cuda.empty_cache()\n",
    "results = {}\n",
    "for i in data2:\n",
    "    results[i] = []\n",
    "\n",
    "for i in tq.tqdm(range(len(data2))):\n",
    "    classe = data2[i]\n",
    "    print(\"Class \"+str(classe)+\"...\")\n",
    "    agent = Agent(classe, alpha=0.2, num_episodes=15, load=True)\n",
    "    res = agent.evaluate(datasets_per_class[classe])\n",
    "    results[classe] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F0hXlnUdhpww"
   },
   "outputs": [],
   "source": [
    "with open('classes_results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHW1aRnAlpzz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B93Yo5QYlp8K",
    "outputId": "94f97893-018d-41e6-fef4-b2b970f35331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "#!pip3 install torch==1.5.1 torchvision==0.6.1 -f https://download.pytorch.org/whl/cu92/torch_stable.html\n",
    "try:\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        LOAD = True\n",
    "        SAVE_MODEL_PATH = '/content/gdrive/MyDrive/models/' + 'q_network'\n",
    "    else:\n",
    "        LOAD = False\n",
    "        SAVE_MODEL_PATH = \"./models/q_network\"\n",
    "except NameError:\n",
    "        LOAD = False\n",
    "        SAVE_MODEL_PATH = \"./models/q_network\"\n",
    "batch_size = 32\n",
    "PATH=\"./datasets/\"\n",
    "\n",
    "batch_size = 32\n",
    "PATH=\"./datasets/\"\n",
    "\n",
    "\n",
    "\n",
    "train_loader2012, val_loader2012 = read_voc_dataset(download=False, year='2012')\n",
    "\n",
    "agents_per_class = {}\n",
    "datasets_per_class = sort_class_extract([ train_loader2012])\n",
    "datasets_eval_per_class = sort_class_extract([ val_loader2012])\n",
    "\n",
    "classes = ['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Xu62Pb68sjcJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "19wEaQK4sjou",
    "outputId": "bd8aaf94-2843-456a-99de-5b08e16c6592"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e0b757785bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs_over_epochs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs_over_epochs'"
     ]
    }
   ],
   "source": [
    "r = open(\"logs_over_epochs\",\"r\")\n",
    "d = r.readlines()\n",
    "d = [i.strip() for i in d]\n",
    "\n",
    "print(d[0])\n",
    "print(d[1])\n",
    "print(d[2])\n",
    "print(d[3])\n",
    "print(d[4])\n",
    "print(d[5])\n",
    "print(list(filter(None, d[1].split(' '))))\n",
    "\n",
    "\n",
    "aps = {}\n",
    "aps[\"0.4\"] = []\n",
    "aps[\"0.5\"] = []\n",
    "aps[\"0.6\"] = []\n",
    "aps[\"0.7\"] = []\n",
    "aps[\"0.8\"] = []\n",
    "\n",
    "recalls = {}\n",
    "recalls[\"0.4\"] = []\n",
    "recalls[\"0.5\"] = []\n",
    "recalls[\"0.6\"] = []\n",
    "recalls[\"0.7\"] = []\n",
    "recalls[\"0.8\"] = []\n",
    "\n",
    "\n",
    "for i in range(1,130, 3):   #(17):\n",
    "    a = list(filter(None, d[i+1].split(' ')))\n",
    "    b = list(filter(None, d[i+2].split(' ')))\n",
    "    c = list(filter(None, d[i+3].split(' ')))\n",
    "    print(\"a : \"+str(a))\n",
    "    print(\"b : \"+str(b))\n",
    "    print(\"c : \"+str(c))\n",
    "    aps[\"0.4\"].append(float(a[1]))\n",
    "    aps[\"0.5\"].append(float(a[2]))\n",
    "    aps[\"0.6\"].append(float(a[3]))\n",
    "    aps[\"0.7\"].append(float(a[4]))\n",
    "    aps[\"0.8\"].append(float(a[5]))\n",
    "\n",
    "    recalls[\"0.4\"].append(float(b[1]))\n",
    "    recalls[\"0.5\"].append(float(b[2]))\n",
    "    recalls[\"0.6\"].append(float(b[3]))\n",
    "    recalls[\"0.7\"].append(float(b[4]))\n",
    "    recalls[\"0.8\"].append(float(b[5]))\n",
    "    \n",
    "ar = []\n",
    "\n",
    "\n",
    "ar = np.array(ar)\n",
    "print(len(plt.plot(ar)))\n",
    "labels = [\"Threshold 0.4\",\"Threshold 0.5\",\"Threshold 0.6\",\"Threshold 0.7\", \"Threshold 0.8\"]\n",
    "colors=['r','g','b','brown','purple']\n",
    "c = 0\n",
    "for key in aps:\n",
    "    sns.plot(aps[key], label=labels[c])\n",
    "    c += 1\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('AP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fabc9713aedfc16047da2bb50b014c91d1a014df217f900cf809c75e8b3fdbd"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ed79390650a49c295cebb9d180a3fb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "101945fb7b344c628b8e680e9f3f63ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3c2c1823ccd406cb79c69888668b0cf",
       "IPY_MODEL_36870e5d110347f8af38ba54545747d7",
       "IPY_MODEL_5b9c78ece7a640a2ba754be44815e940"
      ],
      "layout": "IPY_MODEL_2b0c6c12e938499b8e4c192080be5a9a"
     }
    },
    "120804e7627f44aa8e84cafb494edce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "15ad3cb3a6eb4ab1a88ff498d6819513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1be85b898bc24808a4ce47f2ab4a0e49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2891f0026d6e431c8fc2f6d1999df657": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b0c6c12e938499b8e4c192080be5a9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3630eff3310e4753aadf205630adb301": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36870e5d110347f8af38ba54545747d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1be85b898bc24808a4ce47f2ab4a0e49",
      "max": 1999639040,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6575a82ea898441eac4659ba455ab73b",
      "value": 1999639040
     }
    },
    "3e22a5b915274c8aa93b041d7013c39c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e6c276ba81f45cba58b80a9be929594",
      "placeholder": "​",
      "style": "IPY_MODEL_48fc63be454b42898b08826fd7a58dba",
      "value": " 6/6 [2:11:39&lt;00:00, 1127.55s/it]"
     }
    },
    "48fc63be454b42898b08826fd7a58dba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56d8ed6d49114427a6c858ffe7df003f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b9c78ece7a640a2ba754be44815e940": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_905d068a90ee4d6488082911eae185ba",
      "placeholder": "​",
      "style": "IPY_MODEL_f83afe1dcdb24e1a92cfbeeee3ce6974",
      "value": " 1999639040/1999639040 [02:25&lt;00:00, 14517225.07it/s]"
     }
    },
    "6108f97cd6bf430c9baf48b3d073b230": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eec6f9b2752b44199063de8477cc15e3",
      "placeholder": "​",
      "style": "IPY_MODEL_d5a13ae6c3584de9a824c57b9e339eb8",
      "value": "100%"
     }
    },
    "620f011b1ff94c68980860c1687c496d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6575a82ea898441eac4659ba455ab73b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65ac2d3ca3674e1a8da58e41c534c57c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3630eff3310e4753aadf205630adb301",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_15ad3cb3a6eb4ab1a88ff498d6819513",
      "value": 6
     }
    },
    "69e9daff5f494c63a9f27d02c69cc7ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bbdf022e0674abdb6cde134d673c956": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80c25e1ee2ed4a30953faf72f2fd0d41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87f27b6298c24971b15241e7f680b303": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6108f97cd6bf430c9baf48b3d073b230",
       "IPY_MODEL_65ac2d3ca3674e1a8da58e41c534c57c",
       "IPY_MODEL_3e22a5b915274c8aa93b041d7013c39c"
      ],
      "layout": "IPY_MODEL_7bbdf022e0674abdb6cde134d673c956"
     }
    },
    "8f05fb20b7604c7383d8be1059598812": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69e9daff5f494c63a9f27d02c69cc7ad",
      "placeholder": "​",
      "style": "IPY_MODEL_56d8ed6d49114427a6c858ffe7df003f",
      "value": " 6/6 [13:41&lt;00:00, 116.91s/it]"
     }
    },
    "905d068a90ee4d6488082911eae185ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95ce6449740c4052a6950825c6e24931": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca04b6d3833b442b8fb523c05dff8063",
       "IPY_MODEL_ed1f7bd45f594d5ca1faa5e804616631",
       "IPY_MODEL_8f05fb20b7604c7383d8be1059598812"
      ],
      "layout": "IPY_MODEL_2891f0026d6e431c8fc2f6d1999df657"
     }
    },
    "9e6c276ba81f45cba58b80a9be929594": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a777bcbca46c4e59964e55b4a30e698a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3c2c1823ccd406cb79c69888668b0cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ed79390650a49c295cebb9d180a3fb1",
      "placeholder": "​",
      "style": "IPY_MODEL_a777bcbca46c4e59964e55b4a30e698a",
      "value": "100%"
     }
    },
    "ca04b6d3833b442b8fb523c05dff8063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d68ea7a6295a4c3399f737ce1965d5c4",
      "placeholder": "​",
      "style": "IPY_MODEL_620f011b1ff94c68980860c1687c496d",
      "value": "100%"
     }
    },
    "d5a13ae6c3584de9a824c57b9e339eb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d68ea7a6295a4c3399f737ce1965d5c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed1f7bd45f594d5ca1faa5e804616631": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80c25e1ee2ed4a30953faf72f2fd0d41",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_120804e7627f44aa8e84cafb494edce7",
      "value": 6
     }
    },
    "eec6f9b2752b44199063de8477cc15e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f83afe1dcdb24e1a92cfbeeee3ce6974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
